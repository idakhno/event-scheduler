# Event Scheduler
Producer генерирует события со случайным временем выполнения и отправляет их в Kafka. Consumer читает события из Kafka, проверяет идемпотентность и логирует их в PostgreSQL для последующего анализа, создает отложенные задачи в Redis (Asynq), выполняет задачи в назначенное время, обновляет статусы в PostgreSQL. Система поддерживает масштабирование, обеспечивает отказоустойчивость через ретраи и DLQ (`events-dlq`) для недоставленных сообщений.

## Стек
Все контейнеры ограничены строго на локалку и запускаются на 127.0.0.1, но внутри локальной сети контейнеров видят друг друга.
Ниже описаны составные части сервиса и их порты:
- Go приложения - producer 8080, consumer 8081
- Инфраструктура - Postgres 5432, Kafka 9092, Redis 6379
- Observability - Prometheus 9090, Grafana 3001, Loki 3100, Promtail 9080, Pyroscope 4040
- Экспортеры метрик - Postgres 9187, Redis 9121, Kafka 9308

## Управление окружением
Основные команды для управления окружением:
- Поднять - `docker compose up -d`
- Остановить - `docker compose down`
- Перезапустить - `docker compose restart <нужный контейнер>`
- Удалить данные - `docker compose down -v`

## UI для инфраструктурных приложений
Использованы только стандартные UI сервисов, которые шли в пакете с контейнерами:
- Kafka UI - http://localhost:8083
- Grafana - http://localhost:3001 (логин/пароль из .env)
- Prometheus - http://localhost:9090

## Детали проекта

### Порядок раскатки контейнеров
Сначала поднимаются Postgres, Kafka, Redis и стек наблюдаемости, параллельно, т.к. никто никому не мешает. Затем, после успешных healtcheck Postgres и Kafka, стартует `postgres-migrate`  и `kafka-init`, cоздается схема БД и топики `events` и `events-dlq`, после чего завершают работу. Go-приложения стартуют только после успешных миграций и инициализации Kafka, когда Postgres и Redis уже healthy.

### Миграции
SQL-файлы в `migrations/*.up.sql` и `*.down.sql`, которые применяет init-контейнер `postgres-migrate` на образе `migrate/migrate:latest`. Он запускается автоматически при `docker compose up` после того, как Postgres становится healthy, выполняет `up` и завершается. При необходимости миграции можно прогнать вручную командой `docker compose run --rm postgres-migrate`.

### Наблюдаемость
Метрики всех сервисов собирает Prometheus, дашборды и графики доступны в Grafana. 

Логи стека собираются Promtail и читаются через Loki, которые потом можно посмотреть и фильтрануть по уровням в Grafana в разделе drilldown/logs. 

Для профилирования используется Grafana Pyroscope.

Трейсы не реализовывались, потому что нет http запросов и микросервисов, producer просто тикает в фоне. В будущем можно расширить через Grafana Tempo.

Текущие логи по контейнеру можно смотреть через `docker compose logs -f <service>`. Состояние топиков и сообщений можно посмотреть в Kafka UI.

### Отказоустойчивость
Все сервисы имеют healthcheck и рестарты, ретраи и для Kafka ещё отдельный топик с DLQ для битых сообщений. Для большей отказоустойчивости можно только развернуть K8S окружение, настроить реплики и масштабировать на 3-4 реплики, и поставить circuit-breaker, чтобы отключить плохую ноду и перебалансировать нагрузку на другие реплики, но, на мой взгляд, это выходит за рамки текущего проекта.

### Масштабирование

#### Producer

Запуск нескольких экземпляров через `docker compose up -d --scale producer=N`. Интервал генерации событий меняется в переменной `APP_PRODUCER_INTERVAL`, по умолчанию 2 секунды.

#### Consumer

Максимальное число параллельных consumer ограничено количеством партиций топика `events`. До инициализации в `compose.yml`, в сервисе `kafka-init` измените `--partitions 3` на нужное число партиций, чтобы поддержать нужное число сервисов consumer.

Если Kafka уже работает и в ней есть данные, партиции увеличиваются без остановки командой `docker exec kafka /opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic events --partitions 6`. Уменьшить партиции нельзя, только увеличить.

Аналогичные действия можно выполнить и для топика `events-dlq`, куда попадают недоставленные сообщения.

Настройка параллелизма Asynq через APP_ASYNQ_CONCURRENCY (по умолчанию = количество ядер процессоров).
